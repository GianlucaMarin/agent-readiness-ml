================================================================================
HYPERPARAMETER TUNING - NESTED CROSS-VALIDATION REPORT
================================================================================

Author: Sandra Marin
Date: December 2025

================================================================================
METHODOLOGY
================================================================================

Nested Cross-Validation:
  - Outer Loop: 5-Fold Stratified CV for performance estimation
  - Inner Loop: 3-Fold Stratified Grid Search for hyperparameter selection
  - Stratification: Low (0-30), Medium (30-70), High (70-100)

Total Hyperparameter Combinations Tested: 144

Parameter Grid:
  max_depth: [12, 15, 18, 20]
  min_samples_leaf: [3, 5, 7, 10]
  min_samples_split: [2, 5, 10]
  max_features: ['sqrt', 'log2', 0.5]

Fixed Parameters:
  n_estimators: 200
  random_state: 42
  n_jobs: -1

================================================================================
NESTED CV RESULTS
================================================================================

Outer Fold MAE: 1.172 ± 0.556
CV%: 47.4%

Fold-by-Fold Results:
  Fold 1: MAE=1.177, R²=0.9935, Best_CV_MAE=1.212
  Fold 2: MAE=0.997, R²=0.9961, Best_CV_MAE=1.344
  Fold 3: MAE=2.106, R²=0.9683, Best_CV_MAE=1.161
  Fold 4: MAE=0.932, R²=0.9949, Best_CV_MAE=1.326
  Fold 5: MAE=0.646, R²=0.9959, Best_CV_MAE=1.354

================================================================================
BEST HYPERPARAMETERS
================================================================================

Parameters selected per fold:
 Fold  max_depth max_features  min_samples_leaf  min_samples_split
    1         15          0.5                 3                  5
    2         18          0.5                 3                  2
    3         18         log2                 3                  5
    4         18          0.5                 3                  5
    5         12          0.5                 3                  2

Final Best Hyperparameters (mode across folds):
  max_depth: 18 (selected in 3/5 folds)
  min_samples_leaf: 3 (selected in 5/5 folds)
  min_samples_split: 5 (selected in 3/5 folds)
  max_features: 0.5 (selected in 4/5 folds)

================================================================================
TEST SET PERFORMANCE
================================================================================

Baseline Model:
  Test MAE: 0.636
  Test R²:  0.9959

Tuned Model:
  Test MAE: 0.639
  Test R²:  0.9980

Improvement: -0.5%

================================================================================
FINAL RECOMMENDATION
================================================================================

RECOMMENDATION: KEEP BASELINE MODEL

RATIONALE:
  - Tuned model performs 0.5% WORSE on test set
  - No significant improvement from hyperparameter tuning
  - Baseline hyperparameters are already well-suited
  - Keep using: models/random_forest_initial.joblib

INSIGHT:
  This confirms that the class imbalance problem (9 Low-score samples)
  cannot be solved by hyperparameter optimization alone.
  The limitation is DATA, not MODEL ARCHITECTURE.

================================================================================
END OF REPORT
================================================================================