================================================================================
MODEL COMPARISON SUMMARY: Baseline vs. Weighted vs. Two-Stage
================================================================================

Date: December 2025
Author: Sandra Marin
Project: ML Agent for Website Quality Score Prediction

================================================================================
PROBLEM STATEMENT
================================================================================

Class Imbalance im Training-Datensatz:
  - Low (0-30):     9 samples (7,3%)
  - Medium (30-70): 32 samples (25,8%)
  - High (70-100):  83 samples (66,9%)

Synthetic Blind Test zeigte systematischen Bias:
  - Expected: Low=20%, Medium=40%, High=40%
  - Baseline predicts: Low=2%, Medium=14%, High=84%
  - Average Bias: +3,21 Punkte Überschätzung

ZIEL: Class Imbalance lösen, Low-Score-Predictions verbessern

================================================================================
VERSUCH 1: BASELINE MODEL (Random Forest Regressor)
================================================================================

Architektur:
  - Single RandomForestRegressor
  - n_estimators=200, max_depth=15, min_samples_leaf=3
  - Trainiert auf unbalanced data

Performance:
  Train MAE:  0,591 | R²: 0,9956
  Val MAE:    1,093 | R²: 0,9936
  Test MAE:   0,636 | R²: 0,9921

Performance by Score Range (Test):
  Low (n=2):     MAE = 0,71
  Medium (n=10): MAE = 1,01
  High (n=24):   MAE = 0,48

Synthetic Blind Test:
  Predicted Distribution: Low=1 (2%), Medium=7 (14%), High=42 (84%)
  Mean Prediction: 85,27
  Bias: +3,21 Punkte

ERGEBNIS: ✓ Beste Overall-Performance, aber Class Imbalance nicht gelöst

================================================================================
VERSUCH 2: SAMPLE WEIGHTED MODEL
================================================================================

Architektur:
  - RandomForestRegressor mit sample_weight parameter
  - Balanced weights: Low=4.6x, Medium=1.3x, High=0.5x
  - Ziel: Low-Scores höher gewichten

Performance:
  Train MAE:  0,590 | R²: 0,9956
  Val MAE:    0,958 | R²: 0,9953
  Test MAE:   0,705 | R²: 0,9902

Performance by Score Range (Test):
  Low (n=2):     MAE = 2,29  (+222% vs. Baseline) ❌
  Medium (n=10): MAE = 0,79  (-21,8% vs. Baseline) ✓
  High (n=24):   MAE = 0,49  (+2,1% vs. Baseline) ~

Synthetic Blind Test:
  Predicted Distribution: Low=1 (2%), Medium=7 (14%), High=42 (84%)
  Mean Prediction: 85,03
  Bias: +2,81 Punkte (leicht verbessert)

ERGEBNIS: ✗ Degraded Overall MAE (-10,9%), Class Imbalance NICHT gelöst
          ⚠️ Trade-off: Medium verbessert, aber Low verschlechtert

================================================================================
VERSUCH 3: TWO-STAGE MODEL (SMOTE + Specialized Regressors)
================================================================================

Architektur:
  - Stage 1: RandomForestClassifier (Low/Medium/High)
    - Trainiert auf SMOTE-balanced data (9→83 Low samples)
    - n_estimators=200, max_depth=15

  - Stage 2: Three specialized RandomForestRegressors
    - Low Regressor: Trainiert auf 9 Low-samples (Score 3-29)
    - Medium Regressor: Trainiert auf 32 Medium-samples (Score 31-62)
    - High Regressor: Trainiert auf 83 High-samples (Score 71-100)

Classifier Performance:
  Train Accuracy: 99,2%
  Val Accuracy:   100,0%
  Test Accuracy:  100,0%

Overall Performance:
  Train MAE:  0,591 | R²: 0,9956
  Val MAE:    0,632 | R²: 0,9984
  Test MAE:   0,967 | R²: 0,9915

Performance by Score Range (Test):
  Low (n=2):     MAE = 3,48  (+390% vs. Baseline) ❌❌
  Medium (n=10): MAE = 0,92  (-8,9% vs. Baseline) ⚠️
  High (n=24):   MAE = 0,78  (+62,5% vs. Baseline) ❌

Synthetic Blind Test:
  Predicted Distribution: Low=1 (2%), Medium=5 (10%), High=44 (88%)
  Mean Prediction: 87,05
  Bias: +3,83 Punkte (SCHLECHTER als Baseline!)

ERGEBNIS: ✗✗ FEHLGESCHLAGEN
          - Test MAE -52,1% schlechter als Baseline
          - Class Imbalance NOCH SCHLIMMER (88% High vs. 84%)
          - Trotz perfektem Classifier: schlechte Real-World-Predictions

================================================================================
DIREKTER VERGLEICH
================================================================================

| Modell           | Test MAE | Change | Low MAE | Med MAE | High MAE | Synth Bias |
|------------------|----------|--------|---------|---------|----------|------------|
| Baseline         | 0,636    | -      | 0,71    | 1,01    | 0,48     | +3,21      |
| Sample Weighted  | 0,705    | -10,9% | 2,29    | 0,79    | 0,49     | +2,81      |
| Two-Stage        | 0,967    | -52,1% | 3,48    | 0,92    | 0,78     | +3,83      |

Synthetic Blind Test Distribution (Expected: Low=20%, Med=40%, High=40%):

| Modell           | Low Pred | Med Pred | High Pred | Status           |
|------------------|----------|----------|-----------|------------------|
| Baseline         | 2%       | 14%      | 84%       | Imbalanced       |
| Sample Weighted  | 2%       | 14%      | 84%       | No improvement   |
| Two-Stage        | 2%       | 10%      | 88%       | WORSE            |

================================================================================
ROOT CAUSE ANALYSIS: WARUM SCHEITERTEN DIE LÖSUNGEN?
================================================================================

1. FUNDAMENTAL DATA LIMITATION
   - Nur 9 echte Low-Score Trainings-Samples
   - Zu wenig Diversität für Pattern-Learning
   - Keine ML-Technik kann fehlende Daten kompensieren

2. SAMPLE WEIGHTING: TRADE-OFF PROBLEM
   - Erhöht Gewicht für Low-Samples → verbessert Low-Training-Fit
   - Reduziert Gewicht für High-Samples → verschlechtert High-Generalisierung
   - Mit nur 9 Low-Samples: nicht genug "Signal" für robuste Patterns
   - Ergebnis: Zero-Sum-Game, Overall-Performance verschlechtert

3. SMOTE: INTERPOLATION ≠ NEW DATA
   - SMOTE erstellt synthetische Samples durch k-NN-Interpolation
   - Synthetische Samples haben KEINE neuen Feature-Kombinationen
   - Mit nur 9 Basis-Samples: zu wenig Diversität
   - Classifier lernt SMOTE-Patterns, aber generalisiert nicht zu echten Low-Scores
   - Perfect Test Accuracy (100%) ≠ Good Real-World Performance (88% High Bias)

4. TWO-STAGE: GARBAGE IN, GARBAGE OUT
   - Stage 1 (Classifier): Klassifiziert fast alles als "High"
     - Trainiert auf Features, die 67% High-Scores repräsentieren
     - SMOTE-Samples zu ähnlich zu High-Score-Features
   - Stage 2 (Low Regressor): Overfitted auf nur 9 Trainings-Samples
     - Training MAE: 2,50 (schlecht selbst auf Training!)
     - Test MAE: 3,48 (katastrophal)
   - Kombination verschlimmert Problem: Schlechter Classifier + Overfitted Regressor

5. MATHEMATISCHE GRENZE ERREICHT
   - Mit n=9 Low-Samples: Degrees of Freedom < Features (41)
   - Underdetermined System → keine eindeutige Lösung
   - Jede Technik (Weighting, SMOTE, Specialized Models) stößt an diese Grenze

================================================================================
KEY LEARNINGS
================================================================================

✓ FUNKTIONIERT:
  - Baseline Random Forest für High-Score-Bereich (Score > 70)
  - Feature Importance Analysis (unabhängig von Predictions)
  - Relative Rankings innerhalb High-Quality-Segment

✗ FUNKTIONIERT NICHT:
  - Sample Weighting mit extremer Imbalance (7% Minority)
  - SMOTE mit <10 Minority Samples
  - Two-Stage Models wenn Stage 1 unzuverlässig ist
  - Komplexe Architekturen ohne ausreichende Trainingsdaten

FUNDAMENTALE ERKENNTNIS:
  "No amount of clever modeling can compensate for insufficient data."

  Mathematisch: Mit n=9 Low-Samples und p=41 Features ist n << p
  → Underfitting unvermeidlich, keine Generalisierung möglich

================================================================================
EMPFEHLUNG: BASELINE MODEL BEIBEHALTEN
================================================================================

ENTSCHEIDUNG: random_forest_initial.joblib bleibt Production Model

BEGRÜNDUNG:
  1. Niedrigste Test MAE (0,636)
  2. Beste Overall-Generalisierung (R² = 0,992)
  3. Einfachste Architektur (interpretierbar, wartbar)
  4. Keine zusätzliche Komplexität
  5. Bekannte, dokumentierte Limitationen

LIMITATIONS AKZEPTIERT:
  - Systematic Bias: +3,2 Punkte Überschätzung für Low/Medium
  - Class Imbalance: Predicts 84% High (statt 40%)
  - Low-Score Detection unzuverlässig (nur 2% erkannt)

PRODUCTION USE CASES (empfohlen):
  ✓ High-Quality Website Scoring (Score > 70)
  ✓ Relative Rankings innerhalb High-Segment
  ✓ Feature Importance für Optimierungsempfehlungen
  ✓ Trend-Monitoring für bekannte Websites

PRODUCTION USE CASES (NICHT empfohlen):
  ✗ Low-Quality Website Detection (Score < 40)
  ✗ Erste Evaluation unbekannter Websites
  ✗ Automatische Quality-Gates ohne Human Review
  ✗ Critical Decision-Making basierend auf Single Prediction

================================================================================
ALTERNATIVE LÖSUNGSANSÄTZE (für Zukunft)
================================================================================

Option 1: MEHR DATEN SAMMELN (Empfohlen)
  - Label 50+ zusätzliche Low-Score Websites
  - Label 30+ zusätzliche Medium-Score Websites
  - Ziel: Mindestens 20% pro Kategorie
  - Erwartet: Sample Weighting/SMOTE funktioniert ab n=30+ Minority

Option 2: FEATURE ENGINEERING
  - Neue Features speziell für Low-Score-Charakterisierung
  - Ratio-Features: has_oauth / has_api (Komplexität vs. Basics)
  - Negative Features: missing_* (was fehlt)
  - Domain Knowledge: Expert-Rules für Low-Scores

Option 3: HYBRID-ANSATZ
  - Rule-based Low-Score Detection (< 30 Punkte)
    - Z.B.: If total_features < 5 → Low-Score
  - ML für Medium/High (30-100 Punkte)
    - Baseline Model funktioniert dort gut
  - Ensemble: Rules + ML

Option 4: TRANSFER LEARNING
  - Pre-trained Models auf ähnlichen Domains
  - Fine-tuning auf unserem Dataset
  - Erfordert: Zugang zu größeren Website-Quality-Datasets

Option 5: SEMI-SUPERVISED LEARNING
  - Unsupervised Clustering für Low-Score-Feature-Patterns
  - Supervised Fine-tuning auf gelabelten 9 Samples
  - Erfordert: Große Menge unlabeled Low-Score-Websites

Option 6: AKZEPTANZ
  - Modell als "High-Score-Detector" positionieren
  - Low-Score-Detection = Out-of-Scope
  - Klare Kommunikation der Limitations

================================================================================
FAZIT
================================================================================

Nach drei Modell-Iterationen:
  1. Baseline:        Test MAE 0,636 ✓
  2. Sample Weighted: Test MAE 0,705 ✗ (-10,9%)
  3. Two-Stage:       Test MAE 0,967 ✗✗ (-52,1%)

Ergebnis: BASELINE MODEL IST BESTE LÖSUNG

Lesson Learned:
  "Simple is better than complex when data is limited."

  Komplexere Modelle (Weighting, SMOTE, Two-Stage) können fehlende
  Daten NICHT kompensieren. Bei extremer Class Imbalance (7% Minority)
  ist die einzige echte Lösung: MEHR DATEN SAMMELN.

Production Strategy:
  - Baseline Model für High-Score-Bereich (funktioniert gut)
  - Low-Score-Detection: Manual Review oder Data Collection
  - Transparente Kommunikation der Limitations
  - Continuous Improvement mit mehr gelabelten Daten

================================================================================
ANHANG: GESPEICHERTE MODELLE & OUTPUTS
================================================================================

Models:
  - models/random_forest_initial.joblib       ← PRODUCTION MODEL ✓
  - models/weighted_random_forest.joblib      (nicht empfohlen)
  - models/two_stage_model.joblib             (nicht empfohlen)

Evaluation Outputs:
  - outputs/weighted_model_comparison.png
  - outputs/two_stage_model_evaluation.png
  - data/raw/synthetic_blind_test_50_RESULTS.csv

Source Code:
  - src/train_model.py                        (Baseline)
  - src/train_weighted_model.py               (Sample Weighting)
  - src/train_two_stage_model.py              (SMOTE + Two-Stage)
  - src/predict_synthetic_blind_test.py       (Blind Test Evaluation)

================================================================================
END OF REPORT
================================================================================
