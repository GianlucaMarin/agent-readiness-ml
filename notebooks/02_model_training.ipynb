{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent Readiness ML: Model Training\n",
    "\n",
    "**Phase 2:** Train initial Random Forest and XGBoost models  \n",
    "**Goal:** Beat baseline MAE of 24.04 on validation set  \n",
    "**Strategy:** Start with reasonable hyperparameters, evaluate, then optimize\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import xgboost as xgb\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization settings\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "print(\"‚úì Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## STEP 1: LOAD PREPARED DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"LOADING PREPARED DATA\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Load feature matrices\n",
    "X_train = pd.read_csv('../data/processed/X_train.csv')\n",
    "X_val = pd.read_csv('../data/processed/X_val.csv')\n",
    "X_test = pd.read_csv('../data/processed/X_test.csv')\n",
    "\n",
    "# Load target variables\n",
    "y_train = pd.read_csv('../data/processed/y_train.csv').values.ravel()\n",
    "y_val = pd.read_csv('../data/processed/y_val.csv').values.ravel()\n",
    "y_test = pd.read_csv('../data/processed/y_test.csv').values.ravel()\n",
    "\n",
    "print(\"\\n‚úì Data loaded successfully!\\n\")\n",
    "print(f\"Training Set:\")\n",
    "print(f\"  X_train: {X_train.shape} | y_train: {y_train.shape}\")\n",
    "print(f\"\\nValidation Set:\")\n",
    "print(f\"  X_val:   {X_val.shape} | y_val:   {y_val.shape}\")\n",
    "print(f\"\\nTest Set (HELD OUT):\")\n",
    "print(f\"  X_test:  {X_test.shape} | y_test:  {y_test.shape}\")\n",
    "\n",
    "print(f\"\\nüìã Features: {X_train.shape[1]}\")\n",
    "print(f\"üìä Feature names (first 5): {list(X_train.columns[:5])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## STEP 2: BASELINE PERFORMANCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"BASELINE MODEL: Mean Prediction\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Baseline: predict training mean\n",
    "baseline_pred = np.full(len(y_val), y_train.mean())\n",
    "\n",
    "baseline_mae = mean_absolute_error(y_val, baseline_pred)\n",
    "baseline_rmse = np.sqrt(mean_squared_error(y_val, baseline_pred))\n",
    "baseline_r2 = r2_score(y_val, baseline_pred)\n",
    "\n",
    "print(f\"\\nüìä BASELINE METRICS (Validation Set):\\n\")\n",
    "print(f\"   Strategy: Always predict {y_train.mean():.2f}\")\n",
    "print(f\"   MAE:  {baseline_mae:.2f}\")\n",
    "print(f\"   RMSE: {baseline_rmse:.2f}\")\n",
    "print(f\"   R¬≤:   {baseline_r2:.4f}\")\n",
    "print(f\"\\n   üéØ Goal: Our ML models must beat MAE < {baseline_mae:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## STEP 3: RANDOM FOREST MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Train Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"RANDOM FOREST REGRESSOR\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Initialize model with specified hyperparameters\n",
    "rf_model = RandomForestRegressor(\n",
    "    n_estimators=200,\n",
    "    max_depth=15,\n",
    "    min_samples_leaf=3,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(f\"\\nüå≤ Hyperparameters:\")\n",
    "print(f\"   n_estimators:     {rf_model.n_estimators}\")\n",
    "print(f\"   max_depth:        {rf_model.max_depth}\")\n",
    "print(f\"   min_samples_leaf: {rf_model.min_samples_leaf}\")\n",
    "print(f\"   random_state:     {rf_model.random_state}\")\n",
    "\n",
    "print(f\"\\n‚è≥ Training Random Forest...\")\n",
    "rf_model.fit(X_train, y_train)\n",
    "print(f\"‚úì Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Evaluate Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "rf_train_pred = rf_model.predict(X_train)\n",
    "rf_val_pred = rf_model.predict(X_val)\n",
    "\n",
    "# Metrics - Training Set\n",
    "rf_train_mae = mean_absolute_error(y_train, rf_train_pred)\n",
    "rf_train_rmse = np.sqrt(mean_squared_error(y_train, rf_train_pred))\n",
    "rf_train_r2 = r2_score(y_train, rf_train_pred)\n",
    "\n",
    "# Metrics - Validation Set\n",
    "rf_val_mae = mean_absolute_error(y_val, rf_val_pred)\n",
    "rf_val_rmse = np.sqrt(mean_squared_error(y_val, rf_val_pred))\n",
    "rf_val_r2 = r2_score(y_val, rf_val_pred)\n",
    "\n",
    "print(f\"\\nüìä RANDOM FOREST PERFORMANCE:\\n\")\n",
    "print(f\"   TRAINING SET:\")\n",
    "print(f\"     MAE:  {rf_train_mae:.2f}\")\n",
    "print(f\"     RMSE: {rf_train_rmse:.2f}\")\n",
    "print(f\"     R¬≤:   {rf_train_r2:.4f}\")\n",
    "print(f\"\\n   VALIDATION SET:\")\n",
    "print(f\"     MAE:  {rf_val_mae:.2f}\")\n",
    "print(f\"     RMSE: {rf_val_rmse:.2f}\")\n",
    "print(f\"     R¬≤:   {rf_val_r2:.4f}\")\n",
    "\n",
    "improvement = ((baseline_mae - rf_val_mae) / baseline_mae) * 100\n",
    "print(f\"\\n   üéØ Improvement over baseline: {improvement:.1f}%\")\n",
    "if rf_val_mae < baseline_mae:\n",
    "    print(f\"   ‚úÖ SUCCESS! Beat baseline by {baseline_mae - rf_val_mae:.2f} MAE points\")\n",
    "else:\n",
    "    print(f\"   ‚ùå Did not beat baseline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Random Forest: Predictions vs Actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot: Predicted vs Actual\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Training set\n",
    "axes[0].scatter(y_train, rf_train_pred, alpha=0.6, s=50, edgecolors='black', linewidth=0.5)\n",
    "axes[0].plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], 'r--', lw=2, label='Perfect Prediction')\n",
    "axes[0].set_xlabel('Actual Score', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Predicted Score', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title(f'Random Forest: Training Set\\nMAE={rf_train_mae:.2f}, R¬≤={rf_train_r2:.3f}', fontsize=13, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Validation set\n",
    "axes[1].scatter(y_val, rf_val_pred, alpha=0.6, s=50, edgecolors='black', linewidth=0.5, color='orange')\n",
    "axes[1].plot([y_val.min(), y_val.max()], [y_val.min(), y_val.max()], 'r--', lw=2, label='Perfect Prediction')\n",
    "axes[1].set_xlabel('Actual Score', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Predicted Score', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title(f'Random Forest: Validation Set\\nMAE={rf_val_mae:.2f}, R¬≤={rf_val_r2:.3f}', fontsize=13, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/rf_predictions_vs_actual.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úì Saved: outputs/rf_predictions_vs_actual.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Random Forest: Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract feature importances\n",
    "rf_importances = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': rf_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "# Get top 20\n",
    "top_20_rf = rf_importances.head(20).copy()\n",
    "top_20_rf['Feature_Short'] = top_20_rf['Feature'].str.replace('has_', '')\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(12, 8))\n",
    "bars = plt.barh(range(len(top_20_rf)), top_20_rf['Importance'], color='steelblue', edgecolor='black')\n",
    "\n",
    "# Color by importance\n",
    "for i, bar in enumerate(bars):\n",
    "    if top_20_rf.iloc[i]['Importance'] > 0.08:\n",
    "        bar.set_color('darkgreen')\n",
    "    elif top_20_rf.iloc[i]['Importance'] > 0.05:\n",
    "        bar.set_color('steelblue')\n",
    "    else:\n",
    "        bar.set_color('lightblue')\n",
    "\n",
    "plt.yticks(range(len(top_20_rf)), top_20_rf['Feature_Short'])\n",
    "plt.xlabel('Feature Importance', fontsize=12, fontweight='bold')\n",
    "plt.title('Random Forest: Top 20 Most Important Features', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/rf_feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úì Saved: outputs/rf_feature_importance.png\")\n",
    "print(f\"\\nüîù TOP 10 FEATURES (Random Forest):\\n\")\n",
    "for idx, row in top_20_rf.head(10).iterrows():\n",
    "    print(f\"   {top_20_rf.index.get_loc(idx)+1:2d}. {row['Feature_Short']:30s}  {row['Importance']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## STEP 4: XGBOOST MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Train XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"XGBOOST REGRESSOR\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Initialize model with specified hyperparameters\n",
    "xgb_model = xgb.XGBRegressor(\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=8,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbosity=0\n",
    ")\n",
    "\n",
    "print(f\"\\nüöÄ Hyperparameters:\")\n",
    "print(f\"   n_estimators:  {xgb_model.n_estimators}\")\n",
    "print(f\"   learning_rate: {xgb_model.learning_rate}\")\n",
    "print(f\"   max_depth:     {xgb_model.max_depth}\")\n",
    "print(f\"   random_state:  {xgb_model.random_state}\")\n",
    "\n",
    "print(f\"\\n‚è≥ Training XGBoost...\")\n",
    "xgb_model.fit(X_train, y_train)\n",
    "print(f\"‚úì Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Evaluate XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "xgb_train_pred = xgb_model.predict(X_train)\n",
    "xgb_val_pred = xgb_model.predict(X_val)\n",
    "\n",
    "# Metrics - Training Set\n",
    "xgb_train_mae = mean_absolute_error(y_train, xgb_train_pred)\n",
    "xgb_train_rmse = np.sqrt(mean_squared_error(y_train, xgb_train_pred))\n",
    "xgb_train_r2 = r2_score(y_train, xgb_train_pred)\n",
    "\n",
    "# Metrics - Validation Set\n",
    "xgb_val_mae = mean_absolute_error(y_val, xgb_val_pred)\n",
    "xgb_val_rmse = np.sqrt(mean_squared_error(y_val, xgb_val_pred))\n",
    "xgb_val_r2 = r2_score(y_val, xgb_val_pred)\n",
    "\n",
    "print(f\"\\nüìä XGBOOST PERFORMANCE:\\n\")\n",
    "print(f\"   TRAINING SET:\")\n",
    "print(f\"     MAE:  {xgb_train_mae:.2f}\")\n",
    "print(f\"     RMSE: {xgb_train_rmse:.2f}\")\n",
    "print(f\"     R¬≤:   {xgb_train_r2:.4f}\")\n",
    "print(f\"\\n   VALIDATION SET:\")\n",
    "print(f\"     MAE:  {xgb_val_mae:.2f}\")\n",
    "print(f\"     RMSE: {xgb_val_rmse:.2f}\")\n",
    "print(f\"     R¬≤:   {xgb_val_r2:.4f}\")\n",
    "\n",
    "improvement = ((baseline_mae - xgb_val_mae) / baseline_mae) * 100\n",
    "print(f\"\\n   üéØ Improvement over baseline: {improvement:.1f}%\")\n",
    "if xgb_val_mae < baseline_mae:\n",
    "    print(f\"   ‚úÖ SUCCESS! Beat baseline by {baseline_mae - xgb_val_mae:.2f} MAE points\")\n",
    "else:\n",
    "    print(f\"   ‚ùå Did not beat baseline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 XGBoost: Predictions vs Actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot: Predicted vs Actual\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Training set\n",
    "axes[0].scatter(y_train, xgb_train_pred, alpha=0.6, s=50, edgecolors='black', linewidth=0.5, color='green')\n",
    "axes[0].plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], 'r--', lw=2, label='Perfect Prediction')\n",
    "axes[0].set_xlabel('Actual Score', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Predicted Score', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title(f'XGBoost: Training Set\\nMAE={xgb_train_mae:.2f}, R¬≤={xgb_train_r2:.3f}', fontsize=13, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Validation set\n",
    "axes[1].scatter(y_val, xgb_val_pred, alpha=0.6, s=50, edgecolors='black', linewidth=0.5, color='purple')\n",
    "axes[1].plot([y_val.min(), y_val.max()], [y_val.min(), y_val.max()], 'r--', lw=2, label='Perfect Prediction')\n",
    "axes[1].set_xlabel('Actual Score', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Predicted Score', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title(f'XGBoost: Validation Set\\nMAE={xgb_val_mae:.2f}, R¬≤={xgb_val_r2:.3f}', fontsize=13, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/xgb_predictions_vs_actual.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úì Saved: outputs/xgb_predictions_vs_actual.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 XGBoost: Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract feature importances\n",
    "xgb_importances = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': xgb_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "# Get top 20\n",
    "top_20_xgb = xgb_importances.head(20).copy()\n",
    "top_20_xgb['Feature_Short'] = top_20_xgb['Feature'].str.replace('has_', '')\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(12, 8))\n",
    "bars = plt.barh(range(len(top_20_xgb)), top_20_xgb['Importance'], color='purple', edgecolor='black')\n",
    "\n",
    "# Color by importance\n",
    "for i, bar in enumerate(bars):\n",
    "    if top_20_xgb.iloc[i]['Importance'] > 0.08:\n",
    "        bar.set_color('darkgreen')\n",
    "    elif top_20_xgb.iloc[i]['Importance'] > 0.05:\n",
    "        bar.set_color('purple')\n",
    "    else:\n",
    "        bar.set_color('plum')\n",
    "\n",
    "plt.yticks(range(len(top_20_xgb)), top_20_xgb['Feature_Short'])\n",
    "plt.xlabel('Feature Importance', fontsize=12, fontweight='bold')\n",
    "plt.title('XGBoost: Top 20 Most Important Features', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/xgb_feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úì Saved: outputs/xgb_feature_importance.png\")\n",
    "print(f\"\\nüîù TOP 10 FEATURES (XGBoost):\\n\")\n",
    "for idx, row in top_20_xgb.head(10).iterrows():\n",
    "    print(f\"   {top_20_xgb.index.get_loc(idx)+1:2d}. {row['Feature_Short']:30s}  {row['Importance']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## STEP 5: MODEL COMPARISON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create comparison table\n",
    "comparison = pd.DataFrame({\n",
    "    'Model': ['Baseline (Mean)', 'Random Forest', 'XGBoost'],\n",
    "    'MAE': [baseline_mae, rf_val_mae, xgb_val_mae],\n",
    "    'RMSE': [baseline_rmse, rf_val_rmse, xgb_val_rmse],\n",
    "    'R¬≤': [baseline_r2, rf_val_r2, xgb_val_r2]\n",
    "})\n",
    "\n",
    "print(\"\\nüìä VALIDATION SET PERFORMANCE:\\n\")\n",
    "print(comparison.to_string(index=False))\n",
    "\n",
    "# Find best model\n",
    "best_model_idx = comparison.iloc[1:]['MAE'].idxmin()\n",
    "best_model_name = comparison.iloc[best_model_idx]['Model']\n",
    "best_mae = comparison.iloc[best_model_idx]['MAE']\n",
    "\n",
    "print(f\"\\nüèÜ BEST MODEL: {best_model_name}\")\n",
    "print(f\"   MAE: {best_mae:.2f}\")\n",
    "print(f\"   Improvement over baseline: {((baseline_mae - best_mae) / baseline_mae * 100):.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "metrics = ['MAE', 'RMSE', 'R¬≤']\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
    "\n",
    "for idx, metric in enumerate(metrics):\n",
    "    values = comparison[metric].values\n",
    "    bars = axes[idx].bar(comparison['Model'], values, color=colors[idx], edgecolor='black', linewidth=1.5)\n",
    "    \n",
    "    # Highlight best (lowest MAE/RMSE, highest R¬≤)\n",
    "    if metric in ['MAE', 'RMSE']:\n",
    "        best_idx = np.argmin(values[1:]) + 1\n",
    "    else:\n",
    "        best_idx = np.argmax(values[1:]) + 1\n",
    "    bars[best_idx].set_color('gold')\n",
    "    bars[best_idx].set_edgecolor('darkred')\n",
    "    bars[best_idx].set_linewidth(3)\n",
    "    \n",
    "    axes[idx].set_ylabel(metric, fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_title(f'{metric} Comparison', fontsize=13, fontweight='bold')\n",
    "    axes[idx].grid(True, alpha=0.3, axis='y')\n",
    "    axes[idx].tick_params(axis='x', rotation=15)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        axes[idx].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                      f'{height:.2f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.suptitle('Model Performance Comparison (Validation Set)', fontsize=15, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úì Saved: outputs/model_comparison.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Side-by-side predictions scatter\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Random Forest\n",
    "axes[0].scatter(y_val, rf_val_pred, alpha=0.7, s=60, edgecolors='black', linewidth=0.5, color='steelblue', label='RF Predictions')\n",
    "axes[0].plot([y_val.min(), y_val.max()], [y_val.min(), y_val.max()], 'r--', lw=2, label='Perfect')\n",
    "axes[0].set_xlabel('Actual Score', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Predicted Score', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title(f'Random Forest\\nMAE={rf_val_mae:.2f}, R¬≤={rf_val_r2:.3f}', fontsize=13, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# XGBoost\n",
    "axes[1].scatter(y_val, xgb_val_pred, alpha=0.7, s=60, edgecolors='black', linewidth=0.5, color='purple', label='XGB Predictions')\n",
    "axes[1].plot([y_val.min(), y_val.max()], [y_val.min(), y_val.max()], 'r--', lw=2, label='Perfect')\n",
    "axes[1].set_xlabel('Actual Score', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Predicted Score', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title(f'XGBoost\\nMAE={xgb_val_mae:.2f}, R¬≤={xgb_val_r2:.3f}', fontsize=13, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Predictions vs Actual: Model Comparison (Validation Set)', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/predictions_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úì Saved: outputs/predictions_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## STEP 6: ERROR ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Identify Worst Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ERROR ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Calculate errors\n",
    "rf_errors = np.abs(y_val - rf_val_pred)\n",
    "xgb_errors = np.abs(y_val - xgb_val_pred)\n",
    "\n",
    "# Create error DataFrame\n",
    "error_df = pd.DataFrame({\n",
    "    'Actual': y_val,\n",
    "    'RF_Predicted': rf_val_pred,\n",
    "    'RF_Error': rf_errors,\n",
    "    'XGB_Predicted': xgb_val_pred,\n",
    "    'XGB_Error': xgb_errors\n",
    "})\n",
    "\n",
    "# Random Forest: Top 5 errors\n",
    "print(\"\\nüî¥ RANDOM FOREST - 5 WORST PREDICTIONS:\\n\")\n",
    "rf_worst = error_df.nlargest(5, 'RF_Error')\n",
    "for idx, row in rf_worst.iterrows():\n",
    "    print(f\"   Website #{idx+1}: Actual={row['Actual']:.1f}, Predicted={row['RF_Predicted']:.1f}, Error={row['RF_Error']:.1f}\")\n",
    "\n",
    "# XGBoost: Top 5 errors\n",
    "print(\"\\nüî¥ XGBOOST - 5 WORST PREDICTIONS:\\n\")\n",
    "xgb_worst = error_df.nlargest(5, 'XGB_Error')\n",
    "for idx, row in xgb_worst.iterrows():\n",
    "    print(f\"   Website #{idx+1}: Actual={row['Actual']:.1f}, Predicted={row['XGB_Predicted']:.1f}, Error={row['XGB_Error']:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Error Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze error patterns by score range\n",
    "error_df['Score_Range'] = pd.cut(error_df['Actual'], bins=[0, 30, 60, 80, 100], labels=['Low (0-30)', 'Med (30-60)', 'High (60-80)', 'Very High (80-100)'])\n",
    "\n",
    "print(\"\\nüìä ERROR PATTERNS BY SCORE RANGE:\\n\")\n",
    "print(\"Random Forest:\")\n",
    "print(error_df.groupby('Score_Range')['RF_Error'].agg(['mean', 'max', 'count']))\n",
    "print(\"\\nXGBoost:\")\n",
    "print(error_df.groupby('Score_Range')['XGB_Error'].agg(['mean', 'max', 'count']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Residual Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual plots (errors vs predicted)\n",
    "rf_residuals = y_val - rf_val_pred\n",
    "xgb_residuals = y_val - xgb_val_pred\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Random Forest residuals\n",
    "axes[0].scatter(rf_val_pred, rf_residuals, alpha=0.6, s=50, edgecolors='black', linewidth=0.5, color='steelblue')\n",
    "axes[0].axhline(y=0, color='red', linestyle='--', linewidth=2)\n",
    "axes[0].set_xlabel('Predicted Score', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Residual (Actual - Predicted)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title(f'Random Forest: Residual Plot\\nMean Error={rf_residuals.mean():.2f}', fontsize=13, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# XGBoost residuals\n",
    "axes[1].scatter(xgb_val_pred, xgb_residuals, alpha=0.6, s=50, edgecolors='black', linewidth=0.5, color='purple')\n",
    "axes[1].axhline(y=0, color='red', linestyle='--', linewidth=2)\n",
    "axes[1].set_xlabel('Predicted Score', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Residual (Actual - Predicted)', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title(f'XGBoost: Residual Plot\\nMean Error={xgb_residuals.mean():.2f}', fontsize=13, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/residual_plots.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úì Saved: outputs/residual_plots.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## STEP 7: SAVE MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SAVING MODELS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Save Random Forest\n",
    "rf_metadata = {\n",
    "    'model': rf_model,\n",
    "    'hyperparameters': {\n",
    "        'n_estimators': 200,\n",
    "        'max_depth': 15,\n",
    "        'min_samples_leaf': 3,\n",
    "        'random_state': 42\n",
    "    },\n",
    "    'performance': {\n",
    "        'train_mae': rf_train_mae,\n",
    "        'train_r2': rf_train_r2,\n",
    "        'val_mae': rf_val_mae,\n",
    "        'val_r2': rf_val_r2\n",
    "    },\n",
    "    'feature_names': list(X_train.columns)\n",
    "}\n",
    "joblib.dump(rf_metadata, '../models/random_forest_initial.joblib')\n",
    "print(\"\\n‚úì Saved: models/random_forest_initial.joblib\")\n",
    "print(f\"   Val MAE: {rf_val_mae:.2f}, Val R¬≤: {rf_val_r2:.3f}\")\n",
    "\n",
    "# Save XGBoost\n",
    "xgb_metadata = {\n",
    "    'model': xgb_model,\n",
    "    'hyperparameters': {\n",
    "        'n_estimators': 200,\n",
    "        'learning_rate': 0.05,\n",
    "        'max_depth': 8,\n",
    "        'random_state': 42\n",
    "    },\n",
    "    'performance': {\n",
    "        'train_mae': xgb_train_mae,\n",
    "        'train_r2': xgb_train_r2,\n",
    "        'val_mae': xgb_val_mae,\n",
    "        'val_r2': xgb_val_r2\n",
    "    },\n",
    "    'feature_names': list(X_train.columns)\n",
    "}\n",
    "joblib.dump(xgb_metadata, '../models/xgboost_initial.joblib')\n",
    "print(\"\\n‚úì Saved: models/xgboost_initial.joblib\")\n",
    "print(f\"   Val MAE: {xgb_val_mae:.2f}, Val R¬≤: {xgb_val_r2:.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ MODEL TRAINING COMPLETE!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary & Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nüéØ GOAL: Beat baseline MAE of {baseline_mae:.2f}\")\n",
    "print(f\"\\nüìä RESULTS (Validation Set):\\n\")\n",
    "print(f\"   Baseline:      MAE={baseline_mae:.2f}  R¬≤={baseline_r2:.4f}\")\n",
    "print(f\"   Random Forest: MAE={rf_val_mae:.2f}  R¬≤={rf_val_r2:.4f}  ({((baseline_mae-rf_val_mae)/baseline_mae*100):.1f}% improvement)\")\n",
    "print(f\"   XGBoost:       MAE={xgb_val_mae:.2f}  R¬≤={xgb_val_r2:.4f}  ({((baseline_mae-xgb_val_mae)/baseline_mae*100):.1f}% improvement)\")\n",
    "\n",
    "print(f\"\\nüèÜ WINNER: {best_model_name} (MAE={best_mae:.2f})\")\n",
    "\n",
    "print(f\"\\nüìÅ OUTPUTS GENERATED:\")\n",
    "print(f\"   ‚Ä¢ rf_predictions_vs_actual.png\")\n",
    "print(f\"   ‚Ä¢ xgb_predictions_vs_actual.png\")\n",
    "print(f\"   ‚Ä¢ rf_feature_importance.png\")\n",
    "print(f\"   ‚Ä¢ xgb_feature_importance.png\")\n",
    "print(f\"   ‚Ä¢ model_comparison.png\")\n",
    "print(f\"   ‚Ä¢ predictions_comparison.png\")\n",
    "print(f\"   ‚Ä¢ residual_plots.png\")\n",
    "print(f\"   ‚Ä¢ random_forest_initial.joblib\")\n",
    "print(f\"   ‚Ä¢ xgboost_initial.joblib\")\n",
    "\n",
    "print(f\"\\nüöÄ NEXT STEPS:\")\n",
    "print(f\"   1. Hyperparameter tuning (Grid Search / Random Search)\")\n",
    "print(f\"   2. Feature engineering (interactions, polynomial features)\")\n",
    "print(f\"   3. Ensemble methods (stacking, blending)\")\n",
    "print(f\"   4. Final evaluation on test set\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
